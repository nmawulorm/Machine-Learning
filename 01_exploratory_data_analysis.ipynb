{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. **[Introduction to Exploratory Data Analysis](#Introduction-to-EDA)**\n",
    "1. **[Discovery](#1.-Discovery)**\n",
    "2. **[Structuring](#2.-Structuring)**\n",
    "3. **[Cleaning](#3.-Cleaning)**\n",
    "4. **[Joining](#4.-Joining)**\n",
    "5. **[Validating](#5.-Validating)**\n",
    "6. **[Presenting](#6.-Presenting)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"Introduction-to-EDA\"></a>\n",
    "### Introduction to Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis (EDA) |** The process of investigating, organizing, and analyzing datasets and summarizing their main characteristics, often employing data wrangling and visualization methods.\n",
    "\n",
    "**6 Practices of EDA:**\n",
    "- **Discovering |** process of data familiarization in order to conceptualize how the data can be used\n",
    "- **Structuring |** the process of taking raw data and organizing or transforming it to be more easily visualized, explained, or modeled \n",
    "- **Cleaning |** the process of removing errors that may distort your data or make it less useful\n",
    "- **Joining |** the process of augmenting or adjusting data by adding values from other datasets\n",
    "- **Validating |** the process of verifying that the data is consistent and high quality\n",
    "- **Presenting |** making the cleaned dataset or data visualizations available to others for analysis or further modeling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Discovery\"></a>\n",
    "### 1. Discovery"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Reference Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to ask during the discovery phase:**\n",
    "1. How can I break this data into smaller groups so that I can understand it better?\n",
    "2. How can I prove my hypothesis?\n",
    "3. In its current form, can this data give me the answers I need?\n",
    "\n",
    "**Functions for data discovery:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "| `DataFrame.head()` | The head() method will display the first n rows of the dataframe. <br> In the argument field, input the number of rows you want displayed in a Python notebook. The default is 5 rows. |\n",
    "| `DataFrame.info(X)` | The info() method will display a summary of the dataframe, including the range index, dtypes, column headers, and memory usage.<br> Leaving the argument field blank will return a full summary. As an option, in the argument field you can type in show_counts=True, which will return the count of non-null values for each column. |\n",
    "| `DataFrame.describe()` | The describe() method will return descriptive statistics of the entire dataset, including total count, mean, minimum, maximum, dispersion, and distribution. <br> Leaving the argument field blank will default to returning a summary of the data frame’s statistics. As an option, you can use “include=[X]” and “exclude=[X]” which will limit the results to specific data types, depending on what you input in the brackets. | \n",
    "| `DataFrame.shape` | shape is an attribute that returns a tuple representing the dimensions of the dataframe by number of rows and columns. Remember that attributes are not followed by parentheses. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Code Cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For displaying all of the columns in dataframes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a DataFrame and save in a variable\n",
    "df0 = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gather basic information about the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "df0.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather basic information about the dataset\n",
    "df0.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather descriptive statistics about the data\n",
    "df0.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the size of the dataframe\n",
    "df0.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"2.-Structuring\"></a>\n",
    "### 2. Structuring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Reference Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sorting |** The process of arranging data into meaningful order for analysis\n",
    "\n",
    "**Extracting |** The process of retrieving data from a dataset or source for further processing\n",
    "\n",
    "**Filtering |** The process of selecting a smaller part of your dataset based on specific parameters and using it for viewing or analysis\n",
    "\n",
    "**Slicing |** A method for breaking information down into smaller parts to facilitate efficient examination and analysis from different viewpoints\n",
    "\n",
    "**Grouping |** Aggregating individual observations of a variable into groups\n",
    "\n",
    "\n",
    "**Functions for extracting or selecting data:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "| `df[[columns]]` | Use df[[columns]] to extract/select columns from a dataframe. |\n",
    "| `df.select_dtypes` | A method available to the DataFrame class. <br> Use df.select_dtypes() to return a subset of the dataframe’s columns based on the column dtypes (e.g., float64, int64, bool, object, etc.). |\n",
    "\n",
    "**Functions for filtering, sorting, slicing data:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "| `df[condition]` | Use df[condition] to create a Boolean mask, then apply the mask to the dataframe to filter according to selected condition. | \n",
    "| `pd.sort_values()` | A method available to the DataFrame class. <br> Use pd.sort_values() to sort data according to selected parameters. |\n",
    "| `df.iloc[]` | Use ‘df.iloc[]’ to slice a dataframe based on an integer index location. | \n",
    "| `df.loc[]` | Use df.loc[] to slice a dataframe based on a label or Boolean array. |\n",
    "\n",
    "\n",
    "**Manipulating datetime strings in Python:**\n",
    "| Code | Format | Example |\n",
    "| --- | --- | --- |\n",
    "| `%a` | Abbreviated weekday | Sun |\n",
    "| `%A` | Weekday | Sunday |\n",
    "| `%b` | Abbreviated month | Jan |\n",
    "| `%B` | Month name | January |\n",
    "| `%c` | Date and time | Sun Jan 1 00:00:00 2021 |\n",
    "| `%d` | Day (leading zeros) | 01 to 31 |\n",
    "| `%H` | 24 hours | 00 to 23 |\n",
    "| `%I` | 12 hours | 01 to 12 |\n",
    "| `%j` | Day of year | 001 to 366 |\n",
    "| `%m` | Month | 01 to 12 |\n",
    "| `%M` | Minute | 00 to 59 |\n",
    "| `%p` | AM or PM | AM/PM |\n",
    "| `%S` | Seconds | 00 to 60 |\n",
    "| `%U` | Week number (Sun) | 00 to 53 |\n",
    "| `%W` | Week number (Mon) | 00 to 53 |\n",
    "| `%w` | Weekday | 0 to 6 |\n",
    "| `%x` | Locale’s appropriate date representation | 08/16/88 (None) <br> 08/16/1988 (en_US) <br> 16.08.1988 (de_DE) |\n",
    "| `%X` | A locale’s appropriate time representation | 21:30:00 (en_US) <br> 21:30:00 (de_DE) |\n",
    "| `%y` | Year without century | 00 to 99 |\n",
    "| `%Y` | Year | 2022 |\n",
    "| `%z` | Offset | +0900 |\n",
    "| `%Z` | Time zone | EDT/JST/WET etc (GMT) |\n",
    "\n",
    "**Datetime functions to remember**\n",
    "\n",
    "| Code | Input Type | Input Example | Output Type | Output Example |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| `datetime.strptime(“25/11/2022”, “%d/%m/%Y”)` | string | “25/11/2022” | DateTime | “2022-11-25  00:00:00” |\n",
    "| `datetime.strftime(dt_object, “%d/%m/%Y”)`| DateTime | “2022-11-25  00:00:00” | string | “25/11/2022” |\n",
    "| `dt_object = datetime.strptime(“25/11/2022”, “%d/%m/%Y”)datetime.timestamp(dt_object)` | string | “25/11/2022” | float (UTC timestamp in seconds) | 1617836400.0 |\n",
    "| `datetime.strptime(“25/11/2022”, “%d/%m/%Y”).strftime(“%Y-%m-%d”)` | string | “25/11/2022” | string | “2022-11-25” |\n",
    "| `datetime.fromtimestamp(1617836400.0)` | float (UTC timestamp in seconds) | 1617836400.0 | DateTime | “2022-11-25  00:00:00” |\n",
    "| `datetime.fromtimestamp(1617836400.0).strftime(“%d/%m/%Y”)` | float (UTC timestamp in seconds) | 1617836400.0 | string | “25/11/2022” |\n",
    "| `from pytz import timezone` <br> `ny_time = datetime.strptime(“25-11-2022  09:34:00-0700”, “%d-%m-%Y  %H:%M:%S%f%z”)` <br> `Tokyo_time = ny_time.astimezone(timezone(‘Asia/Tokyo’))` | string | NewYork timezone “25-11-2022  09:34:00-0700” | DateTime | Tokyo timezone “2022-11-25  22:34:00+08:00” |\n",
    "| `datetime.strptime(“20:00”, “%H:%M”).strftime(“%I:%M %p”)` | string | “20:00” | string | “09:00 AM” |\n",
    "| `datetime.strptime(“08:00 PM”, “%I:%M  %p”).strftime(“%H:%M”)` | string | “08:00 PM” | string | “20:00” |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Code Cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"3.-Cleaning\"></a>\n",
    "### 3. Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Reference Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 Missing Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**What to do with missing data:**\n",
    "- Request the missing values to be filled in by the owner of the data\n",
    "- Delete the missing column(s), row(s), or value(s)\n",
    "- Create a NaN category\n",
    "- Derive new representative value(s)\n",
    "    - Forward filling\n",
    "    - Backward filling\n",
    "    - Deriving mean values \n",
    "    - Deriving median values\n",
    "\n",
    "**Useful Functions:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    " `df.info()` | A DataFrame method that returns a concise summary of the dataframe, including a ‘non-null count,’ which helps you know the number of missing values |\n",
    " `pd.isna() / pd.isnull()` | pd.isna() is a pandas function that returns a same-sized Boolean array indicating whether each value is null (you can also use pd.isnull() as an alias). Note that this function also exists as a DataFrame method. |\n",
    " `pd.notna() / pd.notnull()` |  A pandas function that returns a same-sized Boolean array indicating whether each value is NOT null (you can also use pd.notnull() as an alias). Note that this function also exists as a DataFrame method. | \n",
    " `df.fillna()` | A DataFrame method that fills in missing values using specified method |\n",
    " `df.replace()` | A DataFrame method that replaces specified values with other specified values. Can also be applied to pandas Series. | \n",
    " `df.dropna()` | A DataFrame method that removes rows or columns that contain missing values, depending on the axis you specify. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Duplicated Data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Identifying duplicates** \n",
    "A simple way to identify duplicates is to use the  `pd.duplicated()` function from Pandas. This function returns a series of “true/false” outputs, with “true” indicating the data value is a duplicate, and “false” indicating it is a unique value.\n",
    "\n",
    "**Keeping or Dropping Duplicates** Every dataset is unique and you cannot treat every dataset the same. When making the decision on whether to eliminate duplicate values or not, think deeply about the dataset itself and about the objective you wish to achieve. What impact will dropping duplicates have on your dataset and your objective? \n",
    "1. **Deciding to drop |** You should drop or eliminate duplicate values if duplicate values are clearly mistakes or will misrepresent the remaining unique values in the dataset.  \n",
    "2. **Deciding to NOT drop |** You should keep duplicated data in your dataset if the duplicate values are clearly not mistakes and should be taken into account when representing the dataset as a whole. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Outliers\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outliers |** Observations that are an abnormal distance from other values or an overall pattern in a data population\n",
    "\n",
    "**3 Types of Outliers**\n",
    "- Global outliers \n",
    "- Contextual outliers\n",
    "- Collective outliers\n",
    "\n",
    "**Global Outliers |** Values that are completely different from the overall data group and have noa association with any other outliers\n",
    "\n",
    "**Contextual outliers |** Normal data points under certain conditions but become anomalies under most other conditions \n",
    "\n",
    "**Collective outliers |** A group of abnormal point that follow similar patterns and are isolated from the rest of the population \n",
    "\n",
    "**How to handle outliers** \n",
    "\n",
    "It is important to not only detect outliers, but also to have a plan for them.\n",
    "\n",
    "Whether you keep outliers as they are, delete them, or reassign values is a decision that you make on a dataset-by-dataset basis. To help you make the decision, you can start with these general guidelines:\n",
    "\n",
    "- **Delete them**: If you are sure the outliers are mistakes, typos, or errors and the dataset will be used for modeling or machine learning, then you are more likely to decide to delete outliers. Of the three choices, you’ll use this one the least.\n",
    "- **Reassign them**: If the dataset is small and/or the data will be used for modeling or machine learning, you are more likely to choose a path of deriving new values to replace the outlier values.\n",
    "- **Leave them**: For a dataset that you plan to do EDA/analysis on and nothing else, or for a dataset you are preparing for a model that is resistant to outliers, it is most likely that you are going to leave them in.\n",
    "\n",
    "**Useful Functions:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "`df.describe()` | A DataFrame method that returns general statistics about the dataframe which can help determine outliers |\n",
    "`sns.boxplot()` | A seaborn function that generates a box plot. Data points beyond 1.5x the interquartile range are considered outliers. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.4 Categorical and Numeric Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Data |** Data that is divided into a limited number of qualitative groups \n",
    "\n",
    "Data Transformation: \n",
    "\n",
    "1. **Label encoding |** Data transformation technique where each category is assigned a unique number instead of a qualitative value\n",
    "\n",
    "Some potential problems with label encoding:\n",
    "\n",
    "Imagine you’re analyzing a dataset with categories of music genres. You label encode “Blues,” “Electronic Dance Music (EDM),” “Hip Hop,” “Jazz,” “K-Pop,” “Metal,” “ and “Rock,” with the following numeric values, “1, 2, 3, 4, 5, 6, and 7.” \n",
    "\n",
    "With this label encoding, the resulting machine learning model could derive not only a ranking, but also a closer connection between Blues (1) and EDM (2) because of how close they are numerically than, say, Blues(1) and Jazz(4). In addition to these presumed relationships (which you may or may not want in your analysis) you should also notice that each code is equidistant from the other in the numeric sequence, as in 1 to 2 is the same distance as 5 to 6, etc. The question is, does that equidistant relationship accurately represent the relationships between the music genres in your dataset? To ask another question, after encoding, will the visualization or model you build treat the encoded labels as a ranking? \n",
    "\n",
    "The same could be said for the mushroom example above. After label encoding mushroom types, are you satisfied with the fact that the mushrooms are now in a presumed ranked order with button mushrooms ranked first and toadstool ranked eighth? \n",
    "\n",
    "In summary, label encoding may introduce unintended relationships between the categorical data in your dataset. When you are making decisions about label encoding, consider the algorithm you’ll apply to the data and how it may or may not impact label encoded categorical data.\n",
    "\n",
    "2. **One-hot encoding |** Uses *Dummy Variables* with values of 0 or 1, which indicated the presence of absence of something.\n",
    "\n",
    "With this method, we solve the problem of the unintended and problematic relationships that label encoding presented. \n",
    "\n",
    "But one-hot encoding does present its own set of problems, particularly when it comes to logistic and linear regression.\n",
    "\n",
    "**Label encoding or one-hot encoding: How to decide?** \n",
    "\n",
    "There is no simple answer to whether you should use label encoding or one-hot encoding. The decision needs to be made on a case-by-case, or dataset-by-dataset basis. But there are some guidelines to help you. \n",
    "\n",
    "Use label encoding when:\n",
    "- There are a large number of different categorical variables — because label encoding uses far less data than one-hot encoding\n",
    "- The categorical values have a particular order to them (for example, age groups can be grouped as youngest to oldest or oldest to youngest)\n",
    "- You plan to use a decision tree or random forest machine learning model\n",
    "\n",
    "Use one-hot encoding when: \n",
    "- There is a relatively small amount of categorical variables — because one-hot encoding uses much more data than label encoding. \n",
    "- The categorical variables have no particular order\n",
    "- You use a machine learning model in combination with dimensionality reduction (like Principal Component Analysis (PCA))\n",
    "\n",
    "**Useful Functions:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "`df.astype()` | A DataFrame method that allows you to encode its data as a specified dtype. Note that this method can also be used on Series objects.  |\n",
    "`Series.cat.codes` | A Series attribute that returns the numeric category codes of the series. | \n",
    "`pd.get_dummies()` | A function that converts categorical values into new binary columns—one for each different category | \n",
    "`LabelEncoder()` | A transformer from scikit-learn.preprocessing that encodes specified categories or labels with numeric codes. Note that when building predictive models it should only be used on target variables (i.e., y data). |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Code Cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.1 Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all column names\n",
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns as needed\n",
    "df0 = df0.rename(columns={'Work_accident': 'work_accident',\n",
    "                          'average_montly_hours': 'average_monthly_hours',\n",
    "                          'time_spend_company': 'tenure',\n",
    "                          'Department': 'department'})\n",
    "\n",
    "# Display all column names after the update\n",
    "df0.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.2 Check Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df0.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.3 Check Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "duplicates = df0.duplicated().sum()\n",
    "\n",
    "# Percentage of duplicated data\n",
    "percentage = df0.duplicated().sum() / df0.shape[0] * 100\n",
    "\n",
    "print(f'{duplicates} rows contain duplicates amounting to {percentage.round(2)}% of the total data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect some rows containing duplicates as needed\n",
    "df0[df0.duplicated()].head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.3.1 Resolve Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and save resulting dataframe in a new variable as needed\n",
    "df1 = df0.drop_duplicates(keep='first')\n",
    "\n",
    "# Display first few rows of new dataframe as needed\n",
    "df1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.4 Check Outliers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice dictates to check for outliers in all variables of interest to ensure the accuracy and validity of statistical analyses and machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display general statistics about the dataframe which can help determine outliers\n",
    "df1.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.4.1 Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot to visualize distribution of all numeric variables and detect any outliers\n",
    "\n",
    "# plot 1 boxplot for all variables so must first normalize the scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# select numeric columns\n",
    "num_columns = df0[['variable_1', 'variable_2', 'variable_3']]\n",
    "\n",
    "#normalize values using min-max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(num_columns)\n",
    "\n",
    "# Create df with normalized data\n",
    "df_normalized = pd.DataFrame(normalized_data, columns=num_columns.columns)\n",
    "\n",
    "sns.boxplot(data= df_normalized)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.4.2 Outlier investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of rows containing outliers for each variable that needs to be addressed\n",
    "\n",
    "# Compute the 25th percentile value in `X_n`\n",
    "percentile25 = df1['X_n'].quantile(0.25)\n",
    "\n",
    "# Compute the 75th percentile value in `X_n`\n",
    "percentile75 = df1['X_n'].quantile(0.75)\n",
    "\n",
    "# Compute the interquartile range in `X_n`\n",
    "iqr = percentile75 - percentile25\n",
    "\n",
    "# Define the upper limit and lower limit for non-outlier values in `X_n`\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "print(\"Lower limit:\", lower_limit)\n",
    "print(\"Upper limit:\", upper_limit)\n",
    "\n",
    "# Identify subset of data containing outliers in `X_n`\n",
    "outliers = df1[(df1['X_n'] > upper_limit) | (df1['X_n'] < lower_limit)]\n",
    "\n",
    "# Count how many rows in the data contain outliers in `X_n`\n",
    "print(\"Number of rows in the data containing outliers in `X_n`:\", len(outliers))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2.4.3 Outlier Resolution\n",
    "\n",
    "Certain types of models are more sensitive to outliers than others. At the time of model construction, consider whether to remove outliers based on the type of model being used\n",
    "- **Delete them**: If you are sure the outliers are mistakes, typos, or errors and the dataset will be used for modeling or machine learning, then you are more likely to decide to delete outliers. Of the three choices, you’ll use this one the least.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a boolean mask to delete outliers\n",
    "mask = (df['number_of_strikes'] >= lower_limit) & (df['number_of_strikes'] <= upper_limit)\n",
    "\n",
    "df = df[mask].copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Reassign them**: If the dataset is small and/or the data will be used for modeling or machine learning, you are more likely to choose a path of deriving new values to replace the outlier values.\n",
    "    1. **Create a floor and ceiling at a quantile:** For example, you could place walls at the 90th and 10th percentile of the distribution of data values. Any value above the 90% mark or below the 10% mark are changed to fit within the walls you set\n",
    "    2. **Impute the average:** In some cases, it might be best to reassign all outlier values to match the median or mean value. This will ensure that your median and distribution are based solely on the non-outlier values, leaving the original outliers excluded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floor and ceiling method\n",
    "\n",
    "# Calculate 10th percentile\n",
    "tenth_percentile = np.percentile(df['x_n'], 10)\n",
    "\n",
    "# Calculate 90th percentile\n",
    "ninetieth_percentile = np.percentile(df['x_n'], 90)\n",
    "\n",
    "# Apply lambda function to replace outliers with thresholds defined above\n",
    "df['x_n'] = df['x_n'].apply(lambda x: (\n",
    "    tenth_percentile if x < tenth_percentile \n",
    "    else ninetieth_percentile if x > ninetieth_percentile \n",
    "    else x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imputing the average\n",
    "\n",
    "# Calculate median of all NON-OUTLIER values\n",
    "median = np.median(df['number_of_strikes'][df['number_of_strikes'] >= lower_limit])\n",
    "\n",
    "# Impute the median for all values < lower_limit\n",
    "df['number_of_strikes'] = np.where(df['number_of_strikes'] < lower_limit, median, df['number_of_strikes'] )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Leave them**: For a dataset that you plan to do EDA/analysis on and nothing else, or for a dataset you are preparing for a model that is resistant to outliers, it is most likely that you are going to leave them in."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best practice dictates to check for outliers in all variables of interest to ensure the accuracy and validity of statistical analyses and machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.5 Convert Categorical to Numeric Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of columns that need to be encoded\n",
    "columns_to_encode = ['x_1', 'x_2']\n",
    "\n",
    "# instantiate new df from the encoded df\n",
    "df2 = pd.get_dummies(df, columns=columns_to_encode)\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2.6 Check for class imbalance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Class imbalance:** When a data has a predictor variable that contains more instances of one outcome than another\n",
    "\n",
    "**Balancing a Dataset**\n",
    "1. **Downsampling:**  the process of making the minority class represent a larger share of the whole dataset simply by removing observations from the majority class. It is mostly used with datasets that are large. \n",
    "\n",
    "2. **Upsampling:** is the opposite of downsampling, and is done when the dataset doesn’t have a very large number of observations in the first place. Instead of removing observations from the majority class, you increase the number of observations in the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to downsample data use the resample() function from the sklearn.utils module.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate your data into majority and minority classes\n",
    "majority_data = df[df['target_class'] == 0]  # majority class\n",
    "minority_data = df[df['target_class'] == 1]  # minority class\n",
    "\n",
    "# Downsample the majority class\n",
    "downsampled_majority = resample(majority_data, replace=False, n_samples=len(minority_data), random_state=42)\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "downsampled_data = pd.concat([downsampled_majority, minority_data])\n",
    "\n",
    "# Check the class distribution of the downsampled data\n",
    "downsampled_data['target_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To upsample data use the resample() function from the sklearn.utils module.\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate your data into majority and minority classes\n",
    "majority_data = df[df['target_class'] == 0]  # majority class\n",
    "minority_data = df[df['target_class'] == 1]  # minority class\n",
    "\n",
    "# Upsample the minority class\n",
    "upsampled_minority = resample(minority_data, replace=True, n_samples=len(majority_data), random_state=42)\n",
    "\n",
    "# Combine the upsampled minority class with the majority class\n",
    "upsampled_data = pd.concat([majority_data, upsampled_minority])\n",
    "\n",
    "# Check the class distribution of the upsampled data\n",
    "upsampled_data['target_class'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"4.-Joining\"></a>\n",
    "### 4. Joining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Reference Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging |** Method to combine two different data frames along a specified starting column\n",
    "\n",
    "\n",
    "**Functions for combining data:**\n",
    "\n",
    "| Function | Description |\n",
    "| ---- | ---- |\n",
    "`df.merge()` | A method available to the DataFrame class. <br> Use df.merge() to take columns or indices from other dataframes and combine them with the one to which you’re applying the method. |\n",
    "`pd.concat()` | A pandas function to combine series and/or dataframes <br> Use pd.concat() to join columns, rows, or dataframes along a particular axis |\n",
    "`df.join()` | A method available to the DataFrame class. <br> Use df.join() to combine columns with another dataframe either on an index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Code Cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this section to add data as required"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Validating\"></a>\n",
    "### 5. Validating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Reference Guide"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Input Validation |** The practice of thoroughly analyzing and double-checking to make sure data is complete, error-free, and high-quality.\n",
    "\n",
    "**Why validate data?:**\n",
    "- Make more accurate business decisions\n",
    "- Improve complex model performance \n",
    "- Prevent future system crashes, coding issues, or wrong predictions\n",
    "\n",
    "**Questions to ask while validating data:**\n",
    "- Are all entries in the same format?\n",
    "- Are all entries in the same range? \n",
    "- Are the applicable data entries expressed in teh same data type?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Code Cells"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.-Presenting\"></a>\n",
    "### 6. Presenting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Reference Guide\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Data visualization |** Refers to the graphical representation of data and information using visual elements such as charts, graphs, maps, and other visual aids. It is a way of presenting complex data in a visually appealing and easy-to-understand manner, allowing individuals to analyze and interpret patterns, trends, and relationships within the data.\n",
    "\n",
    "**Data visualization serves multiple purposes, including:**\n",
    "\n",
    "- Exploration and analysis: It helps in exploring and analyzing large datasets, enabling users to identify patterns, outliers, correlations, and insights that may not be immediately apparent in raw data.\n",
    "- Communication and storytelling: Visualizing data enhances communication by presenting information in a concise and compelling manner. It allows individuals to effectively convey their findings, narratives, or arguments based on data to others.\n",
    "- Decision-making: By presenting data visually, decision-makers can gain better insights and make informed decisions. Visual representations facilitate understanding and enable stakeholders to grasp complex information quickly.\n",
    "- Identifying trends and patterns: Data visualizations help in identifying trends, patterns, and relationships between variables, enabling businesses and organizations to make data-driven decisions and predictions.\n",
    "- Data exploration and hypothesis testing: Visualizations can aid in exploring data, formulating hypotheses, and testing assumptions. They provide a visual framework to analyze data from different angles and validate or invalidate hypotheses."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Common Graphs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.1 Boxplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Box plots are very useful in visualizing distributions within data\n",
    "- Can be deceiving without the context of the sample sizes that they represent \n",
    "    - Solution is to plot a stacked histogram alongside to visualize the distribution of data in boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "\n",
    "# Create boxplot \n",
    "sns.boxplot(\n",
    "    data=df1, \n",
    "    x='X_n', \n",
    "    y='Dependant Variable', \n",
    "    hue='Dependant Variable',   # specifies the variable from the data to be used for grouping or coloring the histogram bars\n",
    "    orient=\"h\",                 # determines the orientation of the box plot. \"h\" refers to horizontal, so the box plots will be drawn horizontally.\n",
    "    ax=ax[0]                    # specifies the axes object on which the histogram will be drawn. ax[1] refers to the second subplot or axes object.\n",
    ")\n",
    "ax[0].invert_yaxis()            # used to invert the y-axis. This is done to have the dependent variable values displayed in descending order on the y-axis.\n",
    "ax[0].set_title('Title', fontsize='14')\n",
    "\n",
    "# Create histogram showing distribution\n",
    "sns.histplot(\n",
    "    data= df1,\n",
    "    x='Y', \n",
    "    hue='Dependant Variable',   # specifies the variable from the data to be used for grouping or coloring the histogram bars\n",
    "    multiple='dodge',           # determines the method used to handle overlapping bars.'dodge' means the bars are positioned side by side for different values of 'Dependant Variable'\n",
    "    shrink=2,                   # It controls the width of the bars. A higher value like 2 will make the bars narrower, while a lower value would make them wider.\n",
    "    ax=ax[1]                    # specifies the axes object on which the histogram will be drawn. ax[1] refers to the second subplot or axes object.\n",
    ")\n",
    "ax[1].set_title('Title', fontsize='14')\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.2 Histograms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A histogram is a graphical representation that displays the distribution of a continuous variable by dividing the data into intervals (bins) and representing the frequency or count of observations within each bin using vertical bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize = (22,8))\n",
    "\n",
    "# Define filtered data for plot 1\n",
    "df_filter1 = df1[df1['variable'] -- 'conditional statement']\n",
    "\n",
    "# Define long-tenured employees\n",
    "df_filter2 = df1[df1['variable'] -- 'conditional statement']\n",
    "\n",
    "# Plot 1 histogram\n",
    "sns.histplot(\n",
    "    data=, \n",
    "    x='variable name', \n",
    "    hue='variable name', \n",
    "    discrete=1, \n",
    "    hue_order=['1', '2', '3'], \n",
    "    multiple='dodge', \n",
    "    shrink=.5, \n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set_title('Title', fontsize='14')\n",
    "\n",
    "# Plot 2 histogram\n",
    "sns.histplot(\n",
    "    data= , \n",
    "    x='variable name', \n",
    "    hue='variable name', \n",
    "    discrete=1, \n",
    "    hue_order=['1', '2', '3'], \n",
    "    multiple='dodge', \n",
    "    shrink=.5, \n",
    "    ax=ax[0]\n",
    ")\n",
    "ax[0].set_title('Title', fontsize='14')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.3 Scatterplots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A scatter plot is a graphical representation that displays the relationship or correlation between two continuous variables by plotting individual data points as dots on a two-dimensional coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatterplot of `X_1` versus `X_2`\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "sns.scatterplot(\n",
    "    data=df1, \n",
    "    x='X_1', \n",
    "    y='X_2', \n",
    "    hue='variable_x', \n",
    "    alpha=0.4\n",
    ")\n",
    "plt.axvline(x=insert_value, color='#ff6361', label='string_label', ls='--')\n",
    "plt.legend(labels=['name1', 'name2', 'name3'])\n",
    "\n",
    "plt.title('Title', fontsize='14');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2.4 Heatmaps (correlation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks for strong correlations between variables in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a correlation heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "heatmap = sns.heatmap(\n",
    "    data= df0.select_dtypes(include='number').corr(), \n",
    "    vmin=-1, \n",
    "    vmax=1, \n",
    "    annot=True, \n",
    "    cmap=sns.color_palette(\"vlag\", as_cmap=True)\n",
    ")\n",
    "\n",
    "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=10);"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
